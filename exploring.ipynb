{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aecfdf8a-4ee1-4cad-8546-8dc0359bf530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T02:57:42.342181Z",
     "iopub.status.busy": "2025-02-19T02:57:42.341489Z",
     "iopub.status.idle": "2025-02-19T02:57:47.572819Z",
     "shell.execute_reply": "2025-02-19T02:57:47.571555Z",
     "shell.execute_reply.started": "2025-02-19T02:57:42.342112Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14.16s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wherobots\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33de18ec-ddb2-4b24-b524-dd438c75a038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T22:38:44.388016Z",
     "iopub.status.busy": "2025-02-17T22:38:44.387803Z",
     "iopub.status.idle": "2025-02-17T22:38:45.233202Z",
     "shell.execute_reply": "2025-02-17T22:38:45.232736Z",
     "shell.execute_reply.started": "2025-02-17T22:38:44.388000Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c4da2a-04f8-48a4-8744-e215dccff8cc",
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-17T22:39:23.244Z",
     "iopub.execute_input": "2025-02-17T22:39:02.825067Z",
     "iopub.status.busy": "2025-02-17T22:39:02.824820Z"
    }
   },
   "outputs": [],
   "source": [
    "# gdf = gpd.read_parquet('geoparquet/part-00000-170892fe-0fe0-43c1-999d-f0911ce43365-c000.zstd.parquet')\n",
    "# print(gdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b7a63d-b6b4-4667-a97a-e15120513d1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T02:59:08.828801Z",
     "iopub.status.busy": "2025-02-19T02:59:08.828444Z",
     "iopub.status.idle": "2025-02-19T02:59:10.023755Z",
     "shell.execute_reply": "2025-02-19T02:59:10.022867Z",
     "shell.execute_reply.started": "2025-02-19T02:59:08.828761Z"
    }
   },
   "outputs": [],
   "source": [
    "from sedona.spark import SedonaContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9725869e-cc18-4999-89bf-c69b5bec01e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T02:59:10.810776Z",
     "iopub.status.busy": "2025-02-19T02:59:10.810412Z",
     "iopub.status.idle": "2025-02-19T02:59:37.753100Z",
     "shell.execute_reply": "2025-02-19T02:59:37.752529Z",
     "shell.execute_reply.started": "2025-02-19T02:59:10.810738Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with Sedona\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('usa-structures-analysis') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fffad16-d475-4602-bf13-6e5f0d6c639c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:00:03.960247Z",
     "iopub.status.busy": "2025-02-19T03:00:03.959766Z",
     "iopub.status.idle": "2025-02-19T03:00:03.974749Z",
     "shell.execute_reply": "2025-02-19T03:00:03.974142Z",
     "shell.execute_reply.started": "2025-02-19T03:00:03.960206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-7tjpso9rb09tqb:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://kubernetes.default.svc.cluster.local:443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>usa-structures-analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff5c5dec490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abd7a1b-2ad0-4167-817d-b7782c0ccbf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:00:13.586277Z",
     "iopub.status.busy": "2025-02-19T03:00:13.585926Z",
     "iopub.status.idle": "2025-02-19T03:00:13.590530Z",
     "shell.execute_reply": "2025-02-19T03:00:13.589926Z",
     "shell.execute_reply.started": "2025-02-19T03:00:13.586241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the bounding polygon (Minnesota state boundary in WKT format)\n",
    "nyc_boundary = \"\"\"POLYGON((\n",
    "-74.25559 40.49612,\n",
    "-73.70001 40.49612,\n",
    "-73.70001 40.91553,\n",
    "-74.25559 40.91553,\n",
    "-74.25559 40.49612\n",
    "))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ec7a304-574a-423a-b792-b31f4b7bc4eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:04:18.150841Z",
     "iopub.status.busy": "2025-02-19T03:04:18.150473Z",
     "iopub.status.idle": "2025-02-19T03:04:22.054549Z",
     "shell.execute_reply": "2025-02-19T03:04:22.053967Z",
     "shell.execute_reply.started": "2025-02-19T03:04:18.150802Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- BUILD_ID: integer (nullable = true)\n",
      " |-- OCC_CLS: string (nullable = true)\n",
      " |-- PRIM_OCC: string (nullable = true)\n",
      " |-- SEC_OCC: string (nullable = true)\n",
      " |-- PROP_ADDR: string (nullable = true)\n",
      " |-- PROP_CITY: string (nullable = true)\n",
      " |-- PROP_ST: string (nullable = true)\n",
      " |-- PROP_ZIP: string (nullable = true)\n",
      " |-- OUTBLDG: string (nullable = true)\n",
      " |-- HEIGHT: float (nullable = true)\n",
      " |-- SQMETERS: float (nullable = true)\n",
      " |-- SQFEET: float (nullable = true)\n",
      " |-- H_ADJ_ELEV: float (nullable = true)\n",
      " |-- L_ADJ_ELEV: float (nullable = true)\n",
      " |-- FIPS: string (nullable = true)\n",
      " |-- CENSUSCODE: string (nullable = true)\n",
      " |-- PROD_DATE: timestamp (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- USNG: string (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- IMAGE_NAME: string (nullable = true)\n",
      " |-- IMAGE_DATE: timestamp (nullable = true)\n",
      " |-- VAL_METHOD: string (nullable = true)\n",
      " |-- REMARKS: string (nullable = true)\n",
      " |-- UUID: string (nullable = true)\n",
      " |-- bbox: struct (nullable = true)\n",
      " |    |-- xmin: double (nullable = true)\n",
      " |    |-- ymin: double (nullable = true)\n",
      " |    |-- xmax: double (nullable = true)\n",
      " |    |-- ymax: double (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define S3 access parameters\n",
    "source_bucket = \"wherobots\"\n",
    "source_prefix = \"usa-structures/geoparquet/\"\n",
    "endpoint_url = \"https://data.source.coop\"\n",
    "\n",
    "# Configure Spark to access the S3 bucket\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"ASIAUZT33PSS4KOMESBL\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"+et0dMj9EWWXP9MZ0DjsMRM4VOim6i8V0EzH/OMy\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", endpoint_url)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "\n",
    "# Load the dataset directly from the S3 bucket\n",
    "s3_path = f\"s3a://{source_bucket}/{source_prefix}\"\n",
    "df = sedona.read.format(\"geoparquet\").load(s3_path)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dad74f9-e856-4825-92a3-eaeadb47f2b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:04:32.643438Z",
     "iopub.status.busy": "2025-02-19T03:04:32.643017Z",
     "iopub.status.idle": "2025-02-19T03:05:12.158360Z",
     "shell.execute_reply": "2025-02-19T03:05:12.157657Z",
     "shell.execute_reply.started": "2025-02-19T03:04:32.643400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=======================================================> (28 + 1) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+-----+-----------------+------------------+\n",
      "|            geometry|BUILD_ID|HEIGHT| FIPS|         LATITUDE|         LONGITUDE|\n",
      "+--------------------+--------+------+-----+-----------------+------------------+\n",
      "|MULTIPOLYGON (((-...| 7643312|  6.26|36085| 40.5008397017303|-74.24949070512014|\n",
      "|MULTIPOLYGON (((-...| 7643345|  5.98|36085|40.50105967259193| -74.2495003348297|\n",
      "|MULTIPOLYGON (((-...| 7643320|  6.06|36085|40.50088758080822|-74.24931838779386|\n",
      "|MULTIPOLYGON (((-...| 7643321|  5.96|36085|40.50087798650482|-74.24914339496476|\n",
      "|MULTIPOLYGON (((-...| 7643349|   6.2|36085|40.50109100406775|-74.24924744378629|\n",
      "+--------------------+--------+------+-----+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view for SQL queries\n",
    "df.createOrReplaceTempView('structures')\n",
    "\n",
    "# Perform spatial query to find structures within the Minnesota boundary\n",
    "query = f\"\"\"\n",
    "SELECT geometry, BUILD_ID, HEIGHT, FIPS, LATITUDE, LONGITUDE\n",
    "FROM structures\n",
    "WHERE ST_Intersects(geometry, ST_GeomFromWKT('{nyc_boundary}'))\n",
    "\"\"\"\n",
    "\n",
    "result_df = sedona.sql(query)\n",
    "\n",
    "# Show the first few results\n",
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd31427d-72a3-456b-8460-8603036def31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:06:19.628678Z",
     "iopub.status.busy": "2025-02-19T03:06:19.628329Z",
     "iopub.status.idle": "2025-02-19T03:06:51.861047Z",
     "shell.execute_reply": "2025-02-19T03:06:51.860078Z",
     "shell.execute_reply.started": "2025-02-19T03:06:19.628643Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(153 + 1) / 154]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+------------------+-------------------+\n",
      "|summary|          BUILD_ID|           HEIGHT|              FIPS|          LATITUDE|          LONGITUDE|\n",
      "+-------+------------------+-----------------+------------------+------------------+-------------------+\n",
      "|  count|            946575|           946575|            946575|            946575|             946575|\n",
      "|   mean| 6429223.349931067|5.813767657750656| 35217.74953595859| 40.73511725038251| -73.99217570375588|\n",
      "| stddev|2690605.8341995347|4.383484351469974|1010.3790858520778|0.1007636059241613|0.17060791730411276|\n",
      "|    min|            225003|              0.0|             34003|  40.4982668240017|  -74.2574465169539|\n",
      "|    max|           7965563|           219.73|             36119| 40.91779787618529| -73.69933885609143|\n",
      "+-------+------------------+-----------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2d27766-eff1-4c5c-8b42-26badf48e35a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:06:51.863693Z",
     "iopub.status.busy": "2025-02-19T03:06:51.863038Z",
     "iopub.status.idle": "2025-02-19T03:07:12.298626Z",
     "shell.execute_reply": "2025-02-19T03:07:12.297733Z",
     "shell.execute_reply.started": "2025-02-19T03:06:51.863629Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "946575"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94fd84-03f4-4cc6-a534-2d12237e175f",
   "metadata": {},
   "source": [
    "# View buildings with H3 on a `Kepler.gl` map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3b8cf2-c8e9-4c96-901d-28f95c037fa2",
   "metadata": {},
   "source": [
    "## 1. Use Sedona to extract coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36cb88-df7d-42db-81c3-cf403c644dab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:11:44.099332Z",
     "iopub.status.busy": "2025-02-19T03:11:44.098905Z",
     "iopub.status.idle": "2025-02-19T03:11:44.119153Z",
     "shell.execute_reply": "2025-02-19T03:11:44.118590Z",
     "shell.execute_reply.started": "2025-02-19T03:11:44.099296Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# extract lat/lon\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# geometry column is named 'geom'\n",
    "result_df = result_df.withColumn(\"latitude\", expr(\"ST_Y(geometry)\")) \\\n",
    "                     .withColumn(\"longitude\", expr(\"ST_X(geometry)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3aeed-1a31-43dd-87cb-dc3991cf8d8b",
   "metadata": {},
   "source": [
    "## 2. Compute H3 index in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f31092a-d457-4a58-a4a0-db7f368c0c21",
   "metadata": {},
   "source": [
    "### Option 1: use PySpark UDF with h3-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2045f5c-f81a-4e7c-8ba2-fcf9023953d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:22:21.366872Z",
     "iopub.status.busy": "2025-02-19T03:22:21.366135Z",
     "iopub.status.idle": "2025-02-19T03:22:27.066826Z",
     "shell.execute_reply": "2025-02-19T03:22:27.065426Z",
     "shell.execute_reply.started": "2025-02-19T03:22:21.366822Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1493.17s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (4.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install h3\n",
    "import h3\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4228852a-e6f3-409c-bbd3-9f46d31c2fb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:22:27.068795Z",
     "iopub.status.busy": "2025-02-19T03:22:27.068488Z",
     "iopub.status.idle": "2025-02-19T03:22:27.072830Z",
     "shell.execute_reply": "2025-02-19T03:22:27.072217Z",
     "shell.execute_reply.started": "2025-02-19T03:22:27.068758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/wherobots/bin/python\n",
      "['/opt/spark/python', '/tmp/spark-cdcb534d-2ba7-4433-863b-cf5edf590e58/userFiles-ed6335cd-5f22-4cd9-85ae-aacb277d19c8', '/opt/conda/envs/wherobots/lib/python311.zip', '/opt/conda/envs/wherobots/lib/python3.11', '/opt/conda/envs/wherobots/lib/python3.11/lib-dynload', '', '/opt/conda/envs/wherobots/lib/python3.11/site-packages', '/opt/conda/envs/wherobots/lib/python3.11/site-packages/setuptools/_vendor']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09f089ab-8891-4aa0-8fbc-51081a035790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:22:33.650183Z",
     "iopub.status.busy": "2025-02-19T03:22:33.649688Z",
     "iopub.status.idle": "2025-02-19T03:22:33.654468Z",
     "shell.execute_reply": "2025-02-19T03:22:33.653657Z",
     "shell.execute_reply.started": "2025-02-19T03:22:33.650142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2.1\n"
     ]
    }
   ],
   "source": [
    "import h3\n",
    "print(h3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38e05629-7b18-47e1-ab67-930916f1d6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:22:49.890583Z",
     "iopub.status.busy": "2025-02-19T03:22:49.890217Z",
     "iopub.status.idle": "2025-02-19T03:22:55.591519Z",
     "shell.execute_reply": "2025-02-19T03:22:55.590396Z",
     "shell.execute_reply.started": "2025-02-19T03:22:49.890547Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1521.69s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (4.2.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01be5fbb-11ec-492f-bd4c-7ceca5af977a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:23:09.191933Z",
     "iopub.status.busy": "2025-02-19T03:23:09.191527Z",
     "iopub.status.idle": "2025-02-19T03:23:09.210985Z",
     "shell.execute_reply": "2025-02-19T03:23:09.210493Z",
     "shell.execute_reply.started": "2025-02-19T03:23:09.191892Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.47 ms, sys: 452 μs, total: 7.92 ms\n",
      "Wall time: 14.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define UDF (user-defined function) to compute H3 index\n",
    "def lat_lng_to_h3(lat, lng, resolution=8):\n",
    "    return h3.geo_to_h3(lat, lng, resolution) if lat is not None and lng is not None else None\n",
    "\n",
    "# Register UDF in Spark\n",
    "h3_udf = udf(lat_lng_to_h3, StringType())\n",
    "\n",
    "# Apply UDF to Spark DataFrame\n",
    "result_df = result_df.withColumn(\"h3_index\", h3_udf(result_df.latitude, result_df.longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bae94b1a-eeaf-44ab-96b4-b2b531729f56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:23:11.285361Z",
     "iopub.status.busy": "2025-02-19T03:23:11.284579Z",
     "iopub.status.idle": "2025-02-19T03:23:14.767828Z",
     "shell.execute_reply": "2025-02-19T03:23:14.766906Z",
     "shell.execute_reply.started": "2025-02-19T03:23:11.285315Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/19 03:23:14 ERROR TaskSetManager: Task 0 in stage 20.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'h3'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresult_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'h3'\n"
     ]
    }
   ],
   "source": [
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b520d-0564-4012-9ddb-44c3c51ee3b6",
   "metadata": {},
   "source": [
    "### Option 2: Use h3-java via PySpark, better for large data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d89c6f4-f0a4-45ff-aa4b-ef10fe2f32f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:18:29.964857Z",
     "iopub.status.busy": "2025-02-19T03:18:29.964545Z",
     "iopub.status.idle": "2025-02-19T03:18:30.016615Z",
     "shell.execute_reply": "2025-02-19T03:18:30.015916Z",
     "shell.execute_reply.started": "2025-02-19T03:18:29.964820Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# spark.sparkContext.addPyFile(\"h3-java.jar\")\n",
    "\n",
    "# from pyspark.sql.functions import expr\n",
    "\n",
    "# result_df = result_df.withColumn(\"h3_index\", expr(\"h3.geoToH3(ST_Y(geom), ST_X(geom), 8)\"))\n",
    "\n",
    "# THROWS ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d12795-b42e-4366-bd1b-38c95ce55b4c",
   "metadata": {},
   "source": [
    "## 3. aggregate data by h3 index in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c069ae0-06ac-4570-997c-3a73aada3c04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:18:30.018777Z",
     "iopub.status.busy": "2025-02-19T03:18:30.018182Z",
     "iopub.status.idle": "2025-02-19T03:18:30.094317Z",
     "shell.execute_reply": "2025-02-19T03:18:30.093700Z",
     "shell.execute_reply.started": "2025-02-19T03:18:30.018720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.47 ms, sys: 4.01 ms, total: 6.49 ms\n",
      "Wall time: 9.31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "h3_df = result_df.groupBy('h3_index').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc11be1-f82a-4b0a-9843-32a5753e6695",
   "metadata": {},
   "source": [
    "## 4. Convert to Kepler.gl-compatible format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3684502-406d-4f85-a2f0-7fa4bac6b8b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:18:30.095573Z",
     "iopub.status.busy": "2025-02-19T03:18:30.095252Z",
     "iopub.status.idle": "2025-02-19T03:18:32.094718Z",
     "shell.execute_reply": "2025-02-19T03:18:32.093265Z",
     "shell.execute_reply.started": "2025-02-19T03:18:30.095538Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/19 03:18:31 ERROR TaskSetManager: Task 6 in stage 17.0 failed 4 times; aborting job\n",
      "[Stage 17:>                                                      (0 + 10) / 154]\r"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'h3'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%time\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m h3_pd \u001b[38;5;241m=\u001b[39m \u001b[43mh3_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# visualize in Kepler.gl\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeplergl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KeplerGl\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/pandas/conversion.py:202\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[1;32m    205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'h3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                       (0 + 1) / 154]\r"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "h3_pd = h3_df.toPandas()\n",
    "\n",
    "# visualize in Kepler.gl\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "map_kepler = KeplerGl(height=600)\n",
    "map_kepler.add_data(data=h3_pd, name=\"H3 Hexagons\")\n",
    "map_kepler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef3454-3662-4ecd-86dc-f15b63c69307",
   "metadata": {},
   "source": [
    "# Find all buildings that are below 30 meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "076a0ae7-83c4-46f5-8f59-7cf579249e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:26:22.946618Z",
     "iopub.status.busy": "2025-02-19T03:26:22.946138Z",
     "iopub.status.idle": "2025-02-19T03:26:28.648078Z",
     "shell.execute_reply": "2025-02-19T03:26:28.647059Z",
     "shell.execute_reply.started": "2025-02-19T03:26:22.946578Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1734.75s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (4.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1d8311ee-cdee-42f1-94ef-a739a4e13a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:26:37.873313Z",
     "iopub.status.busy": "2025-02-19T03:26:37.872959Z",
     "iopub.status.idle": "2025-02-19T03:26:44.027562Z",
     "shell.execute_reply": "2025-02-19T03:26:44.026457Z",
     "shell.execute_reply.started": "2025-02-19T03:26:37.873273Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1749.68s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache-sedona\n",
      "  Downloading apache_sedona-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: attrs in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from apache-sedona) (25.1.0)\n",
      "Requirement already satisfied: shapely>=1.7.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from apache-sedona) (2.0.4)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from shapely>=1.7.0->apache-sedona) (1.26.4)\n",
      "Downloading apache_sedona-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
      "Installing collected packages: apache-sedona\n",
      "Successfully installed apache-sedona-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install apache-sedona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ef7bbab-f1cb-476f-bfe2-bcaecb2bd2dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:26:55.358840Z",
     "iopub.status.busy": "2025-02-19T03:26:55.358484Z",
     "iopub.status.idle": "2025-02-19T03:26:55.383458Z",
     "shell.execute_reply": "2025-02-19T03:26:55.382764Z",
     "shell.execute_reply.started": "2025-02-19T03:26:55.358801Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/19 03:26:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register H3 functions in Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"H3 Geospatial\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.sedona.sql.SedonaSqlExtensions\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.3.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register H3 functions\n",
    "spark.sql(\"CREATE TEMPORARY FUNCTION geoToH3 AS 'com.uber.h3core.H3Core'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91b13e63-ebf0-46db-bbee-236017554fe3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:27:39.252359Z",
     "iopub.status.busy": "2025-02-19T03:27:39.251937Z",
     "iopub.status.idle": "2025-02-19T03:27:39.275954Z",
     "shell.execute_reply": "2025-02-19T03:27:39.275317Z",
     "shell.execute_reply.started": "2025-02-19T03:27:39.252321Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert geometry to h3 index\n",
    "from pyspark.sql.functions import udf\n",
    "import h3\n",
    "\n",
    "# Define a UDF to compute H3 index\n",
    "def point_to_h3(lat, lon, resolution=8):\n",
    "    return h3.geo_to_h3(lat, lon, resolution) if lat and lon else None\n",
    "\n",
    "# Register as UDF\n",
    "point_to_h3_udf = udf(point_to_h3)\n",
    "\n",
    "# Extract latitude and longitude, then compute H3 index\n",
    "result_df_filtered = result_df_filtered.withColumn(\n",
    "    \"h3_index\", point_to_h3_udf(expr(\"ST_Y(geometry)\"), expr(\"ST_X(geometry)\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b91a2398-7f89-4dd9-944c-ad9b9d3c4aaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:28:00.851130Z",
     "iopub.status.busy": "2025-02-19T03:28:00.850773Z",
     "iopub.status.idle": "2025-02-19T03:28:04.683593Z",
     "shell.execute_reply": "2025-02-19T03:28:04.682699Z",
     "shell.execute_reply.started": "2025-02-19T03:28:00.851092Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/19 03:28:04 ERROR TaskSetManager: Task 0 in stage 21.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'h3'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check if h3 works\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mresult_df_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh3_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'h3'\n"
     ]
    }
   ],
   "source": [
    "# check if h3 works\n",
    "result_df_filtered.select(\"h3_index\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4a900fa-771d-4268-9ed3-e42ff2d1d968",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:25:08.345072Z",
     "iopub.status.busy": "2025-02-19T03:25:08.344683Z",
     "iopub.status.idle": "2025-02-19T03:25:08.363258Z",
     "shell.execute_reply": "2025-02-19T03:25:08.362711Z",
     "shell.execute_reply.started": "2025-02-19T03:25:08.345034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 ms, sys: 0 ns, total: 1.12 ms\n",
      "Wall time: 13.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "result_df_filtered = result_df.filter(result_df.HEIGHT < 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08400d8b-2e99-4cce-bcd9-e38a26261d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:25:32.396552Z",
     "iopub.status.busy": "2025-02-19T03:25:32.396216Z",
     "iopub.status.idle": "2025-02-19T03:25:32.550177Z",
     "shell.execute_reply": "2025-02-19T03:25:32.549337Z",
     "shell.execute_reply.started": "2025-02-19T03:25:32.396510Z"
    }
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_ROUTINE] Cannot resolve function `h3`.`geoToH3` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 1 pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# generate h3 index for each building\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# resoultion 8 is common for visualizing buildings\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expr\n\u001b[0;32m----> 6\u001b[0m result_df_filtered \u001b[38;5;241m=\u001b[39m \u001b[43mresult_df_filtered\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh3_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh3.geoToH3(ST_Y(geom), ST_X(geom), 8)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:5176\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5173\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5174\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5175\u001b[0m     )\n\u001b[0;32m-> 5176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/conda/envs/wherobots/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_ROUTINE] Cannot resolve function `h3`.`geoToH3` on search path [`system`.`builtin`, `system`.`session`, `spark_catalog`.`default`].; line 1 pos 0"
     ]
    }
   ],
   "source": [
    "# generate h3 index for each building\n",
    "# resoultion 8 is common for visualizing buildings\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "result_df_filtered = result_df_filtered.withColumn(\n",
    "    \"h3_index\", expr(\"h3.geoToH3(ST_Y(geom), ST_X(geom), 8)\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae22af-b7fa-492b-b7be-e78488d8d526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cee583-6f2b-41b9-9325-d79d2f3dc358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccceee1-50eb-437c-9436-85cca083d79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e4b65-943b-4ee6-80a7-2c1c2dc403a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2d1a7-68b3-4413-9e8a-8e76fd8e6287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b307b6a-1893-4b23-99e3-8424edc750c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:56:07.006933Z",
     "iopub.status.busy": "2025-02-19T03:56:07.006586Z",
     "iopub.status.idle": "2025-02-19T03:56:07.010952Z",
     "shell.execute_reply": "2025-02-19T03:56:07.010358Z",
     "shell.execute_reply.started": "2025-02-19T03:56:07.006895Z"
    }
   },
   "outputs": [],
   "source": [
    "from sedona.spark import SedonaContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "696907ef-a049-4f7b-b3c3-b026962fa244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:56:08.367107Z",
     "iopub.status.busy": "2025-02-19T03:56:08.366499Z",
     "iopub.status.idle": "2025-02-19T03:56:08.661194Z",
     "shell.execute_reply": "2025-02-19T03:56:08.660281Z",
     "shell.execute_reply.started": "2025-02-19T03:56:08.367068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/19 03:56:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/02/19 03:56:08 WARN UDTRegistration: Cannot register UDT for org.geotools.coverage.grid.GridCoverage2D, which is already registered.\n",
      "25/02/19 03:56:08 WARN SimpleFunctionRegistry: The function rs_union_aggr replaced a previously registered function.\n",
      "25/02/19 03:56:08 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.geom.Geometry, which is already registered.\n",
      "25/02/19 03:56:08 WARN UDTRegistration: Cannot register UDT for org.locationtech.jts.index.SpatialIndex, which is already registered.\n",
      "25/02/19 03:56:08 WARN SimpleFunctionRegistry: The function st_union_aggr replaced a previously registered function.\n",
      "25/02/19 03:56:08 WARN SimpleFunctionRegistry: The function st_envelope_aggr replaced a previously registered function.\n",
      "25/02/19 03:56:08 WARN SimpleFunctionRegistry: The function st_intersection_aggr replaced a previously registered function.\n",
      "25/02/19 03:56:08 WARN SimpleFunctionRegistry: The function st_analyze_aggr replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with Sedona\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('usa-structures-analysis') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fde60684-57ef-4a24-941a-551a0be3bf1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:56:21.392876Z",
     "iopub.status.busy": "2025-02-19T03:56:21.392139Z",
     "iopub.status.idle": "2025-02-19T03:56:21.396644Z",
     "shell.execute_reply": "2025-02-19T03:56:21.395737Z",
     "shell.execute_reply.started": "2025-02-19T03:56:21.392812Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the bounding polygon (Minnesota state boundary in WKT format)\n",
    "nyc_boundary = \"\"\"POLYGON((\n",
    "-74.25559 40.49612,\n",
    "-73.70001 40.49612,\n",
    "-73.70001 40.91553,\n",
    "-74.25559 40.91553,\n",
    "-74.25559 40.49612\n",
    "))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c3cf33a-79ba-487d-af7f-2729b1b9a020",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:56:22.908158Z",
     "iopub.status.busy": "2025-02-19T03:56:22.907781Z",
     "iopub.status.idle": "2025-02-19T03:56:25.084088Z",
     "shell.execute_reply": "2025-02-19T03:56:25.083480Z",
     "shell.execute_reply.started": "2025-02-19T03:56:22.908118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- BUILD_ID: integer (nullable = true)\n",
      " |-- OCC_CLS: string (nullable = true)\n",
      " |-- PRIM_OCC: string (nullable = true)\n",
      " |-- SEC_OCC: string (nullable = true)\n",
      " |-- PROP_ADDR: string (nullable = true)\n",
      " |-- PROP_CITY: string (nullable = true)\n",
      " |-- PROP_ST: string (nullable = true)\n",
      " |-- PROP_ZIP: string (nullable = true)\n",
      " |-- OUTBLDG: string (nullable = true)\n",
      " |-- HEIGHT: float (nullable = true)\n",
      " |-- SQMETERS: float (nullable = true)\n",
      " |-- SQFEET: float (nullable = true)\n",
      " |-- H_ADJ_ELEV: float (nullable = true)\n",
      " |-- L_ADJ_ELEV: float (nullable = true)\n",
      " |-- FIPS: string (nullable = true)\n",
      " |-- CENSUSCODE: string (nullable = true)\n",
      " |-- PROD_DATE: timestamp (nullable = true)\n",
      " |-- SOURCE: string (nullable = true)\n",
      " |-- USNG: string (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- IMAGE_NAME: string (nullable = true)\n",
      " |-- IMAGE_DATE: timestamp (nullable = true)\n",
      " |-- VAL_METHOD: string (nullable = true)\n",
      " |-- REMARKS: string (nullable = true)\n",
      " |-- UUID: string (nullable = true)\n",
      " |-- bbox: struct (nullable = true)\n",
      " |    |-- xmin: double (nullable = true)\n",
      " |    |-- ymin: double (nullable = true)\n",
      " |    |-- xmax: double (nullable = true)\n",
      " |    |-- ymax: double (nullable = true)\n",
      " |-- geohash: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define S3 access parameters\n",
    "source_bucket = \"wherobots\"\n",
    "source_prefix = \"usa-structures/geoparquet/\"\n",
    "endpoint_url = \"https://data.source.coop\"\n",
    "\n",
    "# Configure Spark to access the S3 bucket\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"ASIAUZT33PSS4KOMESBL\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"+et0dMj9EWWXP9MZ0DjsMRM4VOim6i8V0EzH/OMy\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", endpoint_url)\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "\n",
    "# Load the dataset directly from the S3 bucket\n",
    "s3_path = f\"s3a://{source_bucket}/{source_prefix}\"\n",
    "df = sedona.read.format(\"geoparquet\").load(s3_path)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d140aeaa-1f95-40e4-8c77-b638bdb82dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:56:28.564318Z",
     "iopub.status.busy": "2025-02-19T03:56:28.563943Z",
     "iopub.status.idle": "2025-02-19T03:57:01.270465Z",
     "shell.execute_reply": "2025-02-19T03:57:01.269792Z",
     "shell.execute_reply.started": "2025-02-19T03:56:28.564279Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:======================================================> (28 + 1) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+-----+-----------------+------------------+\n",
      "|            geometry|BUILD_ID|HEIGHT| FIPS|         LATITUDE|         LONGITUDE|\n",
      "+--------------------+--------+------+-----+-----------------+------------------+\n",
      "|MULTIPOLYGON (((-...| 7643312|  6.26|36085| 40.5008397017303|-74.24949070512014|\n",
      "|MULTIPOLYGON (((-...| 7643345|  5.98|36085|40.50105967259193| -74.2495003348297|\n",
      "|MULTIPOLYGON (((-...| 7643320|  6.06|36085|40.50088758080822|-74.24931838779386|\n",
      "|MULTIPOLYGON (((-...| 7643321|  5.96|36085|40.50087798650482|-74.24914339496476|\n",
      "|MULTIPOLYGON (((-...| 7643349|   6.2|36085|40.50109100406775|-74.24924744378629|\n",
      "+--------------------+--------+------+-----+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary view for SQL queries\n",
    "df.createOrReplaceTempView('structures')\n",
    "\n",
    "# Perform spatial query to find structures within the Minnesota boundary\n",
    "query = f\"\"\"\n",
    "SELECT geometry, BUILD_ID, HEIGHT, FIPS, LATITUDE, LONGITUDE\n",
    "FROM structures\n",
    "WHERE ST_Intersects(geometry, ST_GeomFromWKT('{nyc_boundary}'))\n",
    "\"\"\"\n",
    "\n",
    "result_df = sedona.sql(query)\n",
    "\n",
    "# Show the first few results\n",
    "result_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ef237a5b-33f1-4adc-8e06-961f99616d99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:57:01.272569Z",
     "iopub.status.busy": "2025-02-19T03:57:01.272246Z",
     "iopub.status.idle": "2025-02-19T03:57:26.383505Z",
     "shell.execute_reply": "2025-02-19T03:57:26.382616Z",
     "shell.execute_reply.started": "2025-02-19T03:57:01.272532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:=====================================================>(153 + 1) / 154]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+-------------------+-------------------+\n",
      "|summary|          BUILD_ID|           HEIGHT|              FIPS|           LATITUDE|          LONGITUDE|\n",
      "+-------+------------------+-----------------+------------------+-------------------+-------------------+\n",
      "|  count|            946575|           946575|            946575|             946575|             946575|\n",
      "|   mean| 6429223.349931067|5.813767657750656| 35217.74953595859|  40.73511725038251| -73.99217570375588|\n",
      "| stddev|2690605.8341995347|4.383484351469974|1010.3790858520778|0.10076360592416671|0.17060791730411207|\n",
      "|    min|            225003|              0.0|             34003|   40.4982668240017|  -74.2574465169539|\n",
      "|    max|           7965563|           219.73|             36119|  40.91779787618529| -73.69933885609143|\n",
      "+-------+------------------+-----------------+------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8bc07116-1c8d-485f-810f-9d580d5922c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:57:26.385798Z",
     "iopub.status.busy": "2025-02-19T03:57:26.385082Z",
     "iopub.status.idle": "2025-02-19T03:57:46.969811Z",
     "shell.execute_reply": "2025-02-19T03:57:46.969266Z",
     "shell.execute_reply.started": "2025-02-19T03:57:26.385728Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "946575"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "35eb9285-cf47-4984-a9ea-f7a9a5aafa97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:57:58.840318Z",
     "iopub.status.busy": "2025-02-19T03:57:58.839932Z",
     "iopub.status.idle": "2025-02-19T03:58:04.777280Z",
     "shell.execute_reply": "2025-02-19T03:58:04.776673Z",
     "shell.execute_reply.started": "2025-02-19T03:57:58.840279Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3630.64s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keplergl in /opt/conda/envs/wherobots/lib/python3.11/site-packages (0.3.7)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/envs/wherobots/lib/python3.11/site-packages (8.1.5)\n",
      "Requirement already satisfied: traittypes>=0.2.1 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (0.2.1)\n",
      "Requirement already satisfied: traitlets>=4.3.2 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (5.14.3)\n",
      "Requirement already satisfied: geopandas>=0.14.3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (1.0.1)\n",
      "Requirement already satisfied: Shapely>=1.6.4.post2 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (2.0.4)\n",
      "Requirement already satisfied: jupyter_packaging>=0.12.3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (0.12.3)\n",
      "Requirement already satisfied: jupyter>=1.0.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (1.1.1)\n",
      "Requirement already satisfied: jupyterlab>=4.1.6 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (4.1.6)\n",
      "Requirement already satisfied: notebook>=6.0.1 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (6.5.4)\n",
      "Requirement already satisfied: pyarrow>=16.0.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (16.0.0)\n",
      "Requirement already satisfied: geoarrow-pyarrow>=0.1.2 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (0.1.2)\n",
      "Requirement already satisfied: geoarrow-pandas>=0.1.1 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from keplergl) (0.1.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipywidgets) (8.32.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from geoarrow-pandas>=0.1.1->keplergl) (2.2.3)\n",
      "Requirement already satisfied: geoarrow-c in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from geoarrow-pyarrow>=0.1.2->keplergl) (0.1.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from geoarrow-pyarrow>=0.1.2->keplergl) (0.6)\n",
      "Requirement already satisfied: numpy>=1.22 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from geopandas>=0.14.3->keplergl) (1.26.4)\n",
      "Requirement already satisfied: pyogrio>=0.7.2 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from geopandas>=0.14.3->keplergl) (0.10.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from geopandas>=0.14.3->keplergl) (24.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from geopandas>=0.14.3->keplergl) (3.7.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: jupyter-console in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter>=1.0.0->keplergl) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter>=1.0.0->keplergl) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter>=1.0.0->keplergl) (6.29.5)\n",
      "Requirement already satisfied: deprecation in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter_packaging>=0.12.3->keplergl) (2.1.0)\n",
      "Requirement already satisfied: setuptools>=60.2.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter_packaging>=0.12.3->keplergl) (75.8.0)\n",
      "Requirement already satisfied: tomlkit in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter_packaging>=0.12.3->keplergl) (0.13.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter_packaging>=0.12.3->keplergl) (0.45.1)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab>=4.1.6->keplergl) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab>=4.1.6->keplergl) (0.27.1)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab>=4.1.6->keplergl) (3.1.5)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab>=4.1.6->keplergl) (5.3.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab>=4.1.6->keplergl) (2.2.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab>=4.1.6->keplergl) (2.10.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.19.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab>=4.1.6->keplergl) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab>=4.1.6->keplergl) (0.2.4)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab>=4.1.6->keplergl) (6.3.2)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from notebook>=6.0.1->keplergl) (26.2.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from notebook>=6.0.1->keplergl) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=5.3.4 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from notebook>=6.0.1->keplergl) (8.2.0)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from notebook>=6.0.1->keplergl) (0.2.0)\n",
      "Requirement already satisfied: nbformat in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from notebook>=6.0.1->keplergl) (5.10.4)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from notebook>=6.0.1->keplergl) (1.6.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from notebook>=6.0.1->keplergl) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from notebook>=6.0.1->keplergl) (0.18.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from notebook>=6.0.1->keplergl) (0.17.0)\n",
      "Requirement already satisfied: nbclassic>=0.4.7 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from notebook>=6.0.1->keplergl) (1.2.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab>=4.1.6->keplergl) (4.8.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab>=4.1.6->keplergl) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab>=4.1.6->keplergl) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab>=4.1.6->keplergl) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from httpx>=0.25.0->jupyterlab>=4.1.6->keplergl) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab>=4.1.6->keplergl) (0.14.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->keplergl) (1.8.12)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->keplergl) (6.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jinja2>=3.0.3->jupyterlab>=4.1.6->keplergl) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter-client>=5.3.4->notebook>=6.0.1->keplergl) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter-core->jupyterlab>=4.1.6->keplergl) (4.3.6)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (0.6.3)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (0.5.3)\n",
      "Requirement already satisfied: overrides in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (7.7.0)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (1.8.0)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab>=4.1.6->keplergl) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab>=4.1.6->keplergl) (0.10.0)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab>=4.1.6->keplergl) (4.23.0)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.19.0->jupyterlab>=4.1.6->keplergl) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->keplergl) (4.13.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->keplergl) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->keplergl) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->keplergl) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->keplergl) (3.1.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->keplergl) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->keplergl) (1.5.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from nbformat->notebook>=6.0.1->keplergl) (2.21.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from pandas->geoarrow-pandas>=0.1.1->keplergl) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from pandas->geoarrow-pandas>=0.1.1->keplergl) (2025.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from argon2-cffi->notebook>=6.0.1->keplergl) (21.2.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: webencodings in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->keplergl) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->keplergl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab>=4.1.6->keplergl) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab>=4.1.6->keplergl) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab>=4.1.6->keplergl) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.19.0->jupyterlab>=4.1.6->keplergl) (0.22.3)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (3.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (6.0.2)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (0.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from python-dateutil>=2.8.2->jupyter-client>=5.3.4->notebook>=6.0.1->keplergl) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab>=4.1.6->keplergl) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.19.0->jupyterlab>=4.1.6->keplergl) (2.3.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=6.0.1->keplergl) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->keplergl) (2.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=6.0.1->keplergl) (2.22)\n",
      "Requirement already satisfied: fqdn in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (3.0.0)\n",
      "Requirement already satisfied: uri-template in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (24.11.1)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/envs/wherobots/lib/python3.11/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab>=4.1.6->keplergl) (2.9.0.20241206)\n"
     ]
    }
   ],
   "source": [
    "!pip install keplergl ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c83f473-a31e-42f4-aabe-d3b1809f1727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T03:58:12.945215Z",
     "iopub.status.busy": "2025-02-19T03:58:12.944848Z",
     "iopub.status.idle": "2025-02-19T03:58:55.471149Z",
     "shell.execute_reply": "2025-02-19T03:58:55.470318Z",
     "shell.execute_reply.started": "2025-02-19T03:58:12.945175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Guide: https://docs.kepler.gl/docs/keplergl-jupyter\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3230534e31064595912fd5a6f972c7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "KeplerGl(data={'structures': {'index': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, …"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keplergl import KeplerGl\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the result_df to a pandas DataFrame\n",
    "result_df_pandas = result_df.select(\"latitude\", \"longitude\", \"BUILD_ID\", \"HEIGHT\", \"FIPS\").toPandas()\n",
    "\n",
    "# Create the Kepler.gl map\n",
    "map_1 = KeplerGl(height=600)\n",
    "\n",
    "# Add the data to the map\n",
    "map_1.add_data(data=result_df_pandas, name=\"structures\")\n",
    "\n",
    "# Display the map inline\n",
    "map_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a82079-0bf2-4f9b-a488-e08c88f15e93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
